{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brand new testing framework. Ideally it will be passed a pipe, some input and a list of expected outputs so that it can test all the steps and the whole pipe????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "should also allow us to run tests using just a function, maybe even just make a pipe of tests and it runs and sees if it all equals if it doesn't equal it errors and returns the one that failed and throws exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the testing framework will test individual pipes and run a series of tests on that pipe,\n",
    "\n",
    "tests are defined as lists with the first input being the input for the pipe and all the subsequent members being the expected outputs at each stage of the pipe, these tests are themselves contained in a list of tests to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "def pretty_print(message, style):\n",
    "    if(style == \"bold\"):\n",
    "        display(Markdown(\"<bold>\" + message + \"</bold>\"))\n",
    "    elif(style == \"red\"):\n",
    "        display(Markdown(\"<font color='red'>\" + message + \"</font>\"))\n",
    "    elif(style == \"green\"):\n",
    "        display(Markdown(\"<font color='green'>\" + message + \"</font>\"))\n",
    "\n",
    "def test(pipe, tests):\n",
    "#         init()\n",
    "        correct_count = 0\n",
    "        wrong_indeces = {}\n",
    "        \n",
    "#         iterate over all three lists at the same time\n",
    "\n",
    "#   if the stage gives the expected output then add one to correct count display nice count of num failed passed at top\n",
    "#   display list of failed steps would be nice if it gave detailed info but will work on that\n",
    "        \n",
    "        for test in tests:\n",
    "    \n",
    "            for i, task in enumerate(pipe.steps):\n",
    "                datum = test[0] \n",
    "                expected = test[i+1]\n",
    "                actual = task(datum)\n",
    "                \n",
    "                if( actual == expected):\n",
    "                    correct_count+= 1\n",
    "\n",
    "                else:\n",
    "                    wrong_indeces[i] = {\"input\": datum, \"expected\": expected, \"actual\": actual}\n",
    "        \n",
    "     \n",
    "        for index in wrong_indeces:\n",
    "            message = \"Failed at stage: {}, actual {}, expected {}, for input {}\".format(index, wrong_indeces[i][\"actual\"],\n",
    "                                                                                        wrong_indeces[i][\"expected\"], datum)\n",
    "            pretty_print(message, \"red\")\n",
    "            \n",
    "        failed_count = (len(pipe.steps) * len(tests)) - correct_count\n",
    "        \n",
    "        pretty_print(\"Tests passed {}, failed {}\".format(correct_count, failed_count), \"green\")\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now streamers can't be introspected and have to be black boxed, individual steps can be pulled out of the class and tested on their own however, not a train smash just need to pull them out and shove them in their own pipe and only feed one thing for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pipe import *\n",
    "\n",
    "def display_message(message):\n",
    "    return message\n",
    "\n",
    "def display_extra(message):\n",
    "    return message + \"hehehehhe\" \n",
    "\n",
    "test1 = Pipe(display_message, display_extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write each test as a list in the order it is expected to occur eg [input, stage 1, stage 2 etc]\n",
    "# make a list of these lists\n",
    "# make a load balancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<font color='green'>Tests passed 4, failed 0</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test(test1, [[\"hi\", 'hi', 'hihehehehhe'], ['bye','bye', 'byehehehehhe']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "issue streamers etc cannot be ingested ATM because they are functions so have no variables stored in them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "# Trick to prevent annoying autosave spamming with messages due to time lags in docker\n",
    "%autosave 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipe import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! you put a raw value at the top of your pipe and you put ['hi', 'bye', 'farewell'] in the opening of the pipe the value at the start of the pipe has been overwritten by the passed in value. You may want to get rid of that value at the top of the pipe to get rid of this message\n",
      "\n",
      "hi\n",
      "hihehehehhe\n",
      "bye\n",
      "byehehehehhe\n",
      "farewell\n",
      "farewellhehehehhe\n",
      "hi\n",
      "hihehehehhe\n",
      "bye\n",
      "byehehehehhe\n",
      "hi\n",
      "hihehehehhe\n",
      "bye\n",
      "byehehehehhe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['hihehehehhe', 'byehehehehhe']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pipe import *\n",
    "\n",
    "def display_message(message):\n",
    "    print(message)\n",
    "    return message\n",
    "\n",
    "def display_extra(message):\n",
    "    print(message + \"hehehehhe\")\n",
    "    return message + \"hehehehhe\" \n",
    "    \n",
    "test1 = Pipe(\n",
    "            [\"hi\", \"bye\"],\n",
    "            stream(\n",
    "                display_message,\n",
    "                display_extra,\n",
    "            )\n",
    "        )\n",
    "test2 = Pipe(\n",
    "     [\"hi\", \"bye\"],\n",
    "            stream(\n",
    "                display_message,\n",
    "                display_extra,\n",
    "            )\n",
    ")\n",
    "\n",
    "test3 = Pipe(\n",
    "        stream(\n",
    "            display_message,\n",
    "            display_extra,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test1.open([\"hi\", \"bye\", \"farewell\"])\n",
    "\n",
    "test2.open()\n",
    "\n",
    "test3.open([\"hi\", \"bye\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Pipe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6fe6653b4441>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m test4 = Pipe(\n\u001b[0m\u001b[1;32m      2\u001b[0m             \u001b[0mdisplay_message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         )\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisplay_extra\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Pipe' is not defined"
     ]
    }
   ],
   "source": [
    "test4 = Pipe(\n",
    "            display_message\n",
    "        )\n",
    "\n",
    "test4.append(display_extra)\n",
    "\n",
    "test4.open(\"hi\")\n",
    "\n",
    "test5 = Pipe(\n",
    "            display_message\n",
    "        )\n",
    "\n",
    "test5.replace(0, display_extra)\n",
    "\n",
    "test5.open(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello1\n",
      "goodbye1\n",
      "hello2\n",
      "goodbye2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'hi1': 1, 'hi2': 2}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hello_1():\n",
    "    return 1\n",
    "\n",
    "def say_hello(num):\n",
    "    print(\"hello\" + str(num))\n",
    "    return num\n",
    "\n",
    "def say_goodbye(num):\n",
    "    print(\"goodbye\" + str(num))\n",
    "    return num\n",
    "    \n",
    "test_pipe1 = Pipe(\n",
    "                hello_1,\n",
    "                say_hello,\n",
    "                say_goodbye,\n",
    "                name = 'hi1'\n",
    "            )\n",
    "\n",
    "def hello_2():\n",
    "    return 2\n",
    "\n",
    "test_pipe2 = Pipe(\n",
    "                hello_2,\n",
    "                say_hello,\n",
    "                say_goodbye,\n",
    "                name = 'hi2'\n",
    "            )\n",
    "\n",
    "running = parallel(test_pipe1, test_pipe2)\n",
    "join(running)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pytest' from '/opt/conda/lib/python3.6/site-packages/pytest.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pytest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider making each test a function that returns a equality operator, this gets passed to a test pipe, test pipe just tests each function one by one adds up passes which would be true and falses which are fails and records failed indexes\n",
    "\n",
    "diplays lists of errors hopefully with test names, expected and actual???\n",
    "\n",
    "says how many failed and how many passed\n",
    "\n",
    "functions just return tuples like ( actual, expected, actual == expected )  get funcnt name by retrieving .__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
